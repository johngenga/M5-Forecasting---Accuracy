{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa72d24-4668-4815-a7ac-79360a3e9b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Train Validation Data:\n",
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
      "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
      "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
      "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
      "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
      "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
      "\n",
      "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
      "0       1       3       0       1       1  \n",
      "1       1       0       0       0       0  \n",
      "2       1       0       1       1       1  \n",
      "3       0       1       3       7       2  \n",
      "4       1       2       2       2       4  \n",
      "\n",
      "[5 rows x 1919 columns]\n",
      "\n",
      "Sales Train Evaluation Data:\n",
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
      "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
      "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
      "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
      "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
      "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
      "\n",
      "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
      "0       0       3       3       0       1  \n",
      "1       0       0       0       0       0  \n",
      "2       0       2       3       0       1  \n",
      "3       1       3       0       2       6  \n",
      "4       0       0       2       1       0  \n",
      "\n",
      "[5 rows x 1947 columns]\n",
      "\n",
      "Calendar Data:\n",
      "         date  wm_yr_wk    weekday  wday  month  year    d event_name_1  \\\n",
      "0  2011-01-29     11101   Saturday     1      1  2011  d_1          NaN   \n",
      "1  2011-01-30     11101     Sunday     2      1  2011  d_2          NaN   \n",
      "2  2011-01-31     11101     Monday     3      1  2011  d_3          NaN   \n",
      "3  2011-02-01     11101    Tuesday     4      2  2011  d_4          NaN   \n",
      "4  2011-02-02     11101  Wednesday     5      2  2011  d_5          NaN   \n",
      "\n",
      "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
      "0          NaN          NaN          NaN        0        0        0  \n",
      "1          NaN          NaN          NaN        0        0        0  \n",
      "2          NaN          NaN          NaN        0        0        0  \n",
      "3          NaN          NaN          NaN        1        1        0  \n",
      "4          NaN          NaN          NaN        1        0        1  \n",
      "\n",
      "Sell Prices Data:\n",
      "  store_id        item_id  wm_yr_wk  sell_price\n",
      "0     CA_1  HOBBIES_1_001     11325        9.58\n",
      "1     CA_1  HOBBIES_1_001     11326        9.58\n",
      "2     CA_1  HOBBIES_1_001     11327        8.26\n",
      "3     CA_1  HOBBIES_1_001     11328        8.26\n",
      "4     CA_1  HOBBIES_1_001     11329        8.26\n",
      "\n",
      "Sample Submission Data:\n",
      "                              id  F1  F2  F3  F4  F5  F6  F7  F8  F9  ...  \\\n",
      "0  HOBBIES_1_001_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
      "1  HOBBIES_1_002_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
      "2  HOBBIES_1_003_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
      "3  HOBBIES_1_004_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
      "4  HOBBIES_1_005_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
      "\n",
      "   F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n",
      "0    0    0    0    0    0    0    0    0    0    0  \n",
      "1    0    0    0    0    0    0    0    0    0    0  \n",
      "2    0    0    0    0    0    0    0    0    0    0  \n",
      "3    0    0    0    0    0    0    0    0    0    0  \n",
      "4    0    0    0    0    0    0    0    0    0    0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "Sales Train Validation Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30490 entries, 0 to 30489\n",
      "Columns: 1919 entries, id to d_1913\n",
      "dtypes: int64(1913), object(6)\n",
      "memory usage: 446.4+ MB\n",
      "None\n",
      "\n",
      "Sales Train Evaluation Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30490 entries, 0 to 30489\n",
      "Columns: 1947 entries, id to d_1941\n",
      "dtypes: int64(1941), object(6)\n",
      "memory usage: 452.9+ MB\n",
      "None\n",
      "\n",
      "Calendar Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          1969 non-null   object\n",
      " 1   wm_yr_wk      1969 non-null   int64 \n",
      " 2   weekday       1969 non-null   object\n",
      " 3   wday          1969 non-null   int64 \n",
      " 4   month         1969 non-null   int64 \n",
      " 5   year          1969 non-null   int64 \n",
      " 6   d             1969 non-null   object\n",
      " 7   event_name_1  162 non-null    object\n",
      " 8   event_type_1  162 non-null    object\n",
      " 9   event_name_2  5 non-null      object\n",
      " 10  event_type_2  5 non-null      object\n",
      " 11  snap_CA       1969 non-null   int64 \n",
      " 12  snap_TX       1969 non-null   int64 \n",
      " 13  snap_WI       1969 non-null   int64 \n",
      "dtypes: int64(7), object(7)\n",
      "memory usage: 215.5+ KB\n",
      "None\n",
      "\n",
      "Sell Prices Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6841121 entries, 0 to 6841120\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   store_id    object \n",
      " 1   item_id     object \n",
      " 2   wm_yr_wk    int64  \n",
      " 3   sell_price  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 208.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the common path to the files.\n",
    "data_path = 'C:/Users/User/PycharmProjects/M5 Forecasting - Accuracy/data/'\n",
    "\n",
    "# Load datasets using the common path\n",
    "sales_train_validation = pd.read_csv(f'{data_path}sales_train_validation.csv')\n",
    "sales_train_evaluation = pd.read_csv(f'{data_path}sales_train_evaluation.csv')\n",
    "calendar = pd.read_csv(f'{data_path}calendar.csv')\n",
    "sell_prices = pd.read_csv(f'{data_path}sell_prices.csv')\n",
    "sample_submission = pd.read_csv(f'{data_path}sample_submission.csv')\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"Sales Train Validation Data:\")\n",
    "print(sales_train_validation.head())\n",
    "\n",
    "print(\"\\nSales Train Evaluation Data:\")\n",
    "print(sales_train_evaluation.head())\n",
    "\n",
    "print(\"\\nCalendar Data:\")\n",
    "print(calendar.head())\n",
    "\n",
    "print(\"\\nSell Prices Data:\")\n",
    "print(sell_prices.head())\n",
    "\n",
    "print(\"\\nSample Submission Data:\")\n",
    "print(sample_submission.head())\n",
    "\n",
    "# Display data types and check for missing values\n",
    "print(\"\\nSales Train Validation Data Info:\")\n",
    "print(sales_train_validation.info())\n",
    "\n",
    "print(\"\\nSales Train Evaluation Data Info:\")\n",
    "print(sales_train_evaluation.info())\n",
    "\n",
    "print(\"\\nCalendar Data Info:\")\n",
    "print(calendar.info())\n",
    "\n",
    "print(\"\\nSell Prices Data Info:\")\n",
    "print(sell_prices.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df824325-ac6b-40e2-9f24-5dad2427c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melted Sales Train Validation Data:\n",
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales  \n",
      "0       CA  d_1      0  \n",
      "1       CA  d_1      0  \n",
      "2       CA  d_1      0  \n",
      "3       CA  d_1      0  \n",
      "4       CA  d_1      0  \n"
     ]
    }
   ],
   "source": [
    "# Melt the sales_train_validation data\n",
    "sales_train_validation_melted = sales_train_validation.melt(\n",
    "    id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "    var_name='d',\n",
    "    value_name='sales'\n",
    ")\n",
    "\n",
    "# Display the first few rows of the melted DataFrame\n",
    "print(\"Melted Sales Train Validation Data:\")\n",
    "print(sales_train_validation_melted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59f89f4-48e5-4079-ba56-ed13d49521d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/39 merged\n",
      "Chunk 2/39 merged\n",
      "Chunk 3/39 merged\n",
      "Chunk 4/39 merged\n",
      "Chunk 5/39 merged\n",
      "Chunk 6/39 merged\n",
      "Chunk 7/39 merged\n",
      "Chunk 8/39 merged\n",
      "Chunk 9/39 merged\n",
      "Chunk 10/39 merged\n",
      "Chunk 11/39 merged\n",
      "Chunk 12/39 merged\n",
      "Chunk 13/39 merged\n",
      "Chunk 14/39 merged\n",
      "Chunk 15/39 merged\n",
      "Chunk 16/39 merged\n",
      "Chunk 17/39 merged\n",
      "Chunk 18/39 merged\n",
      "Chunk 19/39 merged\n",
      "Chunk 20/39 merged\n",
      "Chunk 21/39 merged\n",
      "Chunk 22/39 merged\n",
      "Chunk 23/39 merged\n",
      "Chunk 24/39 merged\n",
      "Chunk 25/39 merged\n",
      "Chunk 26/39 merged\n",
      "Chunk 27/39 merged\n",
      "Chunk 28/39 merged\n",
      "Chunk 29/39 merged\n",
      "Chunk 30/39 merged\n",
      "Chunk 31/39 merged\n",
      "Chunk 32/39 merged\n",
      "Chunk 33/39 merged\n",
      "Chunk 34/39 merged\n",
      "Chunk 35/39 merged\n",
      "Chunk 36/39 merged\n",
      "Chunk 37/39 merged\n",
      "Chunk 38/39 merged\n",
      "Chunk 39/39 merged\n",
      "Merged Sales Train Validation Data:\n",
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales        date  wm_yr_wk  ... month  year  event_name_1  \\\n",
      "0       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n",
      "1       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n",
      "2       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n",
      "3       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n",
      "4       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n",
      "\n",
      "   event_type_1 event_name_2 event_type_2 snap_CA snap_TX  snap_WI  sell_price  \n",
      "0           NaN          NaN          NaN       0       0        0         NaN  \n",
      "1           NaN          NaN          NaN       0       0        0         NaN  \n",
      "2           NaN          NaN          NaN       0       0        0         NaN  \n",
      "3           NaN          NaN          NaN       0       0        0         NaN  \n",
      "4           NaN          NaN          NaN       0       0        0         NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "def merge_in_chunks(df, calendar, prices, chunk_size=1500000):\n",
    "    # List to hold the merged chunks\n",
    "    merged_chunks = []\n",
    "    \n",
    "    # Number of chunks\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        chunk = df[start:end]\n",
    "        chunk = chunk.merge(calendar, on='d', how='left')\n",
    "        chunk = chunk.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "              \n",
    "        merged_chunks.append(chunk)\n",
    "        \n",
    "        print(f\"Chunk {i+1}/{num_chunks} merged\")\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    merged_df = pd.concat(merged_chunks, ignore_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Merge the melted DataFrame in chunks\n",
    "sales_train_validation_merged = merge_in_chunks(sales_train_validation_melted, calendar, sell_prices)\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "print(\"Merged Sales Train Validation Data:\")\n",
    "print(sales_train_validation_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37b1648-5e5b-4bc0-a3b0-b89a97001792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing sell prices with the last available price\n",
    "sales_train_validation_merged['sell_price'] = sales_train_validation_merged['sell_price'].ffill()\n",
    "\n",
    "# Fill missing event data with 'None'\n",
    "event_cols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "sales_train_validation_merged[event_cols] = sales_train_validation_merged[event_cols].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21ff0c6-c03b-4536-a686-e6d3a6d2724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, window_sizes):\n",
    "    for window in window_sizes:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby(['id'])['sales'].shift(1).rolling(window=window).mean()\n",
    "        df[f'rolling_std_{window}'] = df.groupby(['id'])['sales'].shift(1).rolling(window=window).std()\n",
    "    return df\n",
    "\n",
    "# Apply rolling features\n",
    "window_sizes = [7, 30]\n",
    "sales_train_validation_merged = create_rolling_features(sales_train_validation_merged, window_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c953bb-dacb-445d-881f-039ef62af713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaNs in rolling features\n",
    "rolling_mean_cols = [f'rolling_mean_{window}' for window in window_sizes]\n",
    "rolling_std_cols = [f'rolling_std_{window}' for window in window_sizes]\n",
    "\n",
    "# Fill NaNs in rolling features with the overall mean and std (assuming that initial periods can use overall stats)\n",
    "sales_train_validation_merged[rolling_mean_cols] = sales_train_validation_merged[rolling_mean_cols].fillna(sales_train_validation_merged['sales'].mean())\n",
    "sales_train_validation_merged[rolling_std_cols] = sales_train_validation_merged[rolling_std_cols].fillna(sales_train_validation_merged['sales'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123ce9d0-025a-4d3d-806a-b19a9c0a1d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id        item_id    dept_id   cat_id store_id  \\\n",
      "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
      "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
      "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
      "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
      "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
      "\n",
      "  state_id    d  sales        date  wm_yr_wk  ... event_name_2  event_type_2  \\\n",
      "0       CA  d_1      0  2011-01-29     11101  ...         None          None   \n",
      "1       CA  d_1      0  2011-01-29     11101  ...         None          None   \n",
      "2       CA  d_1      0  2011-01-29     11101  ...         None          None   \n",
      "3       CA  d_1      0  2011-01-29     11101  ...         None          None   \n",
      "4       CA  d_1      0  2011-01-29     11101  ...         None          None   \n",
      "\n",
      "   snap_CA  snap_TX snap_WI sell_price rolling_mean_7 rolling_std_7  \\\n",
      "0        0        0       0        NaN       1.126322      3.873108   \n",
      "1        0        0       0        NaN       1.126322      3.873108   \n",
      "2        0        0       0        NaN       1.126322      3.873108   \n",
      "3        0        0       0        NaN       1.126322      3.873108   \n",
      "4        0        0       0        NaN       1.126322      3.873108   \n",
      "\n",
      "   rolling_mean_30  rolling_std_30  \n",
      "0         1.126322        3.873108  \n",
      "1         1.126322        3.873108  \n",
      "2         1.126322        3.873108  \n",
      "3         1.126322        3.873108  \n",
      "4         1.126322        3.873108  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "#Select and identify departments for splitting.\n",
    "departments = sales_train_validation_merged['dept_id'].unique()\n",
    "dept = departments[0]\n",
    "dept_data = sales_train_validation_merged[sales_train_validation_merged['dept_id'] == dept]\n",
    "\n",
    "# Display the first few rows of the filtered data\n",
    "print(dept_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fc4131-ec43-495c-ad00-0279e2329c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sell_price  wm_yr_wk  snap_CA  snap_TX  snap_WI  rolling_mean_7  \\\n",
      "0         NaN     11101        0        0        0        1.126322   \n",
      "1         NaN     11101        0        0        0        1.126322   \n",
      "2         NaN     11101        0        0        0        1.126322   \n",
      "3         NaN     11101        0        0        0        1.126322   \n",
      "4         NaN     11101        0        0        0        1.126322   \n",
      "\n",
      "   rolling_mean_30  rolling_std_7  rolling_std_30  \n",
      "0         1.126322       3.873108        3.873108  \n",
      "1         1.126322       3.873108        3.873108  \n",
      "2         1.126322       3.873108        3.873108  \n",
      "3         1.126322       3.873108        3.873108  \n",
      "4         1.126322       3.873108        3.873108  \n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: sales, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define the features and target\n",
    "features = ['sell_price', 'wm_yr_wk', 'snap_CA', 'snap_TX', 'snap_WI', 'rolling_mean_7', 'rolling_mean_30', 'rolling_std_7', 'rolling_std_30']\n",
    "target = 'sales'\n",
    "\n",
    "# Split features and target\n",
    "X_dept = dept_data[features]\n",
    "y_dept = dept_data[target]\n",
    "\n",
    "# Display the first few rows of features and target\n",
    "print(X_dept.head())\n",
    "print(y_dept.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05403d96-d1af-436a-9981-795e9efc0dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6366464, 9) (1591616, 9)\n",
      "(6366464,) (1591616,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dept, y_dept, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "print(X_train.shape, X_val.shape)\n",
    "print(y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82f9d7e-a9ff-48a0-9cea-d96e8d8a3811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sell_price  wm_yr_wk   snap_CA   snap_TX   snap_WI  rolling_mean_7  \\\n",
      "0   -0.167930  1.241998 -0.700768 -0.700611 -0.700711       -0.622069   \n",
      "1   -0.920401  0.044922  1.427006 -0.700611  1.427123       -0.780357   \n",
      "2   -0.022226  0.058223 -0.700768 -0.700611 -0.700711        0.011082   \n",
      "3   -0.371516 -1.371616  1.427006 -0.700611 -0.700711       -0.780357   \n",
      "4    0.231260 -1.531226 -0.700768 -0.700611 -0.700711       -0.780357   \n",
      "\n",
      "   rolling_mean_30  rolling_std_7  rolling_std_30  \n",
      "0        -0.529157      -0.496201        0.008745  \n",
      "1         0.085068      -0.695858        0.446821  \n",
      "2        -0.529157      -0.033670       -0.540083  \n",
      "3        -0.473318      -0.695858       -0.212309  \n",
      "4        -0.417480      -0.695858       -0.184708  \n"
     ]
    }
   ],
   "source": [
    "# Standardize the data before training the model.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Display the first few rows of the scaled training data\n",
    "print(pd.DataFrame(X_train_scaled, columns=features).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911405bc-f3cc-4d1e-b400-fba3bc959424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X_train_scaled after filling: 0\n",
      "NaNs in X_val_scaled after filling: 0\n"
     ]
    }
   ],
   "source": [
    "# Fill NaNs in X_train_scaled with the mean of the respective columns\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=features)\n",
    "X_train_scaled.fillna(X_train_scaled.mean(), inplace=True)\n",
    "\n",
    "# Fill NaNs in X_val_scaled with the mean of the respective columns\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=features)\n",
    "X_val_scaled.fillna(X_val_scaled.mean(), inplace=True)\n",
    "\n",
    "# Convert back to numpy arrays\n",
    "X_train_scaled = X_train_scaled.values\n",
    "X_val_scaled = X_val_scaled.values\n",
    "\n",
    "# Check for NaNs again\n",
    "print(\"NaNs in X_train_scaled after filling:\", np.isnan(X_train_scaled).sum())\n",
    "print(\"NaNs in X_val_scaled after filling:\", np.isnan(X_val_scaled).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b49cac-63b0-48f1-8931-e10e9c6cdaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32452405 -0.00147733 -0.02324319  0.00679424 -0.00946815  0.59432778\n",
      "  0.20606699 -0.40566879 -0.11468987]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the model\n",
    "model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 10000  # Adjust based on your capacity\n",
    "num_chunks = int(np.ceil(X_train_scaled.shape[0] / chunk_size))\n",
    "\n",
    "# Train the model in chunks\n",
    "for i in range(num_chunks):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = min((i + 1) * chunk_size, X_train_scaled.shape[0])\n",
    "    \n",
    "    X_chunk = X_train_scaled[start_index:end_index]\n",
    "    y_chunk = y_train[start_index:end_index]\n",
    "    \n",
    "    model.partial_fit(X_chunk, y_chunk)\n",
    "\n",
    "# Save the model and scaler for the current department\n",
    "models = {}\n",
    "models[dept] = (model, scaler)\n",
    "\n",
    "# Check the model coefficients\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb01996-e699-44d3-b7e3-652e8d398e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for department HOBBIES_1: 5.140489398514257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate and print the mean squared error\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Mean Squared Error for department {dept}: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b232f55-acf9-4ded-acbb-7ce3b596dcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for department HOBBIES_1: 5.150490329802295\n",
      "Mean Squared Error for department HOBBIES_2: 0.5389023739198509\n",
      "Mean Squared Error for department HOUSEHOLD_1: 6.888095461524832\n",
      "Mean Squared Error for department HOUSEHOLD_2: 0.6066900174136646\n",
      "Mean Squared Error for department FOODS_1: 8.371112126519412\n",
      "Mean Squared Error for department FOODS_2: 6.635525609995429\n",
      "Mean Squared Error for department FOODS_3: 35.45667945635838\n",
      "All departments processed and models stored.\n"
     ]
    }
   ],
   "source": [
    "models = {}  # Dictionary to store models and scalers for each department\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Get unique department IDs\n",
    "dept_ids = sales_train_validation_merged['dept_id'].unique()\n",
    "\n",
    "for dept in dept_ids:\n",
    "    # Filter data for the current department\n",
    "    dept_data = sales_train_validation_merged[sales_train_validation_merged['dept_id'] == dept]\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = dept_data[features]\n",
    "    y = dept_data[target]\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Handle NaNs\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features)\n",
    "    X_train_scaled.fillna(X_train_scaled.mean(), inplace=True)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled, columns=features)\n",
    "    X_val_scaled.fillna(X_val_scaled.mean(), inplace=True)\n",
    "    X_train_scaled = X_train_scaled.values\n",
    "    X_val_scaled = X_val_scaled.values\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "    \n",
    "    # Define the chunk size\n",
    "    chunk_size = 10000  # Adjust based on your capacity\n",
    "    num_chunks = int(np.ceil(X_train_scaled.shape[0] / chunk_size))\n",
    "    \n",
    "    # Train the model in chunks\n",
    "    for i in range(num_chunks):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = min((i + 1) * chunk_size, X_train_scaled.shape[0])\n",
    "        \n",
    "        X_chunk = X_train_scaled[start_index:end_index]\n",
    "        y_chunk = y_train[start_index:end_index]\n",
    "        \n",
    "        model.partial_fit(X_chunk, y_chunk)\n",
    "    \n",
    "    # Store the model and scaler for the current department\n",
    "    models[dept] = (model, scaler)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(f\"Mean Squared Error for department {dept}: {mse}\")\n",
    "\n",
    "print(\"All departments processed and models stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03ac1f84-a047-43a7-9fc6-06db6287c4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  wm_yr_wk    weekday  wday  month  year       d event_name_1  \\\n",
      "0 2016-04-25     11613     Monday     3      4  2016  d_1914          NaN   \n",
      "1 2016-04-26     11613    Tuesday     4      4  2016  d_1915          NaN   \n",
      "2 2016-04-27     11613  Wednesday     5      4  2016  d_1916          NaN   \n",
      "3 2016-04-28     11613   Thursday     6      4  2016  d_1917          NaN   \n",
      "4 2016-04-29     11613     Friday     7      4  2016  d_1918          NaN   \n",
      "\n",
      "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
      "0          NaN          NaN          NaN        0        0        0  \n",
      "1          NaN          NaN          NaN        0        0        0  \n",
      "2          NaN          NaN          NaN        0        0        0  \n",
      "3          NaN          NaN          NaN        0        0        0  \n",
      "4          NaN          NaN          NaN        0        0        0  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'date' column to datetime\n",
    "calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "# Generate dates for the next 28 days starting from 2016-04-25\n",
    "start_date = '2016-04-25'\n",
    "next_28_days_dates = pd.date_range(start=start_date, periods=28)\n",
    "\n",
    "# Create a DataFrame for the next 28 days\n",
    "next_28_days = pd.DataFrame({\n",
    "    'date': next_28_days_dates\n",
    "})\n",
    "# Merge with calendar data to get required features\n",
    "next_28_days = next_28_days.merge(calendar, how='left', on='date')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(next_28_days.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fd48840-756c-4121-a9ec-e769f818eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your features list, excluding non-numeric columns\n",
    "features = [\n",
    "    'sell_price', 'wm_yr_wk', 'snap_CA', 'snap_TX', 'snap_WI',\n",
    "    'rolling_mean_7', 'rolling_mean_30', 'rolling_std_7', 'rolling_std_30'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74deae64-12f0-4479-9a6c-624bcd9163f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_next_28_days(dept_id, model, scaler, start_date, calendar, features):\n",
    "    # Convert the date columns to datetime64[ns]\n",
    "    calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "\n",
    "    # Generate dates for the next 28 days\n",
    "    next_28_days_dates = pd.date_range(start=start_date, periods=28)\n",
    "\n",
    "    # Create a DataFrame for the next 28 days\n",
    "    next_28_days = pd.DataFrame({\n",
    "        'date': next_28_days_dates\n",
    "    })\n",
    "\n",
    "    # Merge with calendar data to get required features\n",
    "    next_28_days = next_28_days.merge(calendar, how='left', on='date')\n",
    "\n",
    "    # Ensure all required features are present\n",
    "    for feature in features:\n",
    "        if feature not in next_28_days.columns:\n",
    "            next_28_days[feature] = np.nan\n",
    "\n",
    "    # Fill any NaNs with zero\n",
    "    next_28_days.fillna(0, inplace=True)\n",
    "    \n",
    "    # Select only the necessary features\n",
    "    next_28_days_features = next_28_days[features]\n",
    "\n",
    "    # Scale the features\n",
    "    next_28_days_features_scaled = scaler.transform(next_28_days_features)\n",
    "\n",
    "    # Predict sales\n",
    "    predictions = model.predict(next_28_days_features_scaled)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec668b91-b5f8-417d-acfa-1fcbc4027d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19384\\3703920161.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  next_28_days.fillna(0, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19384\\3703920161.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  next_28_days.fillna(0, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19384\\3703920161.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  next_28_days.fillna(0, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19384\\3703920161.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  next_28_days.fillna(0, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19384\\3703920161.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  next_28_days.fillna(0, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19384\\3703920161.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  next_28_days.fillna(0, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19384\\3703920161.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  next_28_days.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming models and scalers are already defined and available\n",
    "dept_ids = sales_train_validation_merged['dept_id'].unique()\n",
    "models = {}  # Load or define your models here\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for dept in dept_ids:\n",
    "    # Get the model and scaler for the current department  \n",
    "    models[dept] = (model, scaler)    \n",
    "    # Forecast the next 28 days\n",
    "    predictions = forecast_next_28_days(dept, model, scaler, start_date, calendar, features)\n",
    "    \n",
    "    # Create a DataFrame for the predictions\n",
    "    dept_predictions = pd.DataFrame({\n",
    "        'id': [f'{dept}_validation'],\n",
    "        **{f'F{i + 1}': predictions[i] for i in range(28)}\n",
    "    })    \n",
    "    all_predictions.append(dept_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bd6fbc9-02fa-414f-9ee9-a690bcae4b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts in final predictions: id     0\n",
      "F1     0\n",
      "F2     0\n",
      "F3     0\n",
      "F4     0\n",
      "F5     0\n",
      "F6     0\n",
      "F7     0\n",
      "F8     0\n",
      "F9     0\n",
      "F10    0\n",
      "F11    0\n",
      "F12    0\n",
      "F13    0\n",
      "F14    0\n",
      "F15    0\n",
      "F16    0\n",
      "F17    0\n",
      "F18    0\n",
      "F19    0\n",
      "F20    0\n",
      "F21    0\n",
      "F22    0\n",
      "F23    0\n",
      "F24    0\n",
      "F25    0\n",
      "F26    0\n",
      "F27    0\n",
      "F28    0\n",
      "dtype: int64\n",
      "                       id        F1        F2        F3        F4        F5  \\\n",
      "0    HOBBIES_1_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "0    HOBBIES_2_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "0  HOUSEHOLD_1_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "0  HOUSEHOLD_2_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "0      FOODS_1_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "\n",
      "         F6        F7        F8        F9  ...       F19       F20       F21  \\\n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "\n",
      "        F22       F23       F24       F25       F26       F27       F28  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine all department predictions into one DataFrame\n",
    "final_predictions = pd.concat(all_predictions, axis=0)\n",
    "\n",
    "# Check for NaNs in the final predictions\n",
    "nan_counts_final = final_predictions.isna().sum()\n",
    "print(\"NaN counts in final predictions:\", nan_counts_final)\n",
    "\n",
    "# Fill NaNs with 0 or any appropriate value\n",
    "final_predictions.fillna(0, inplace=True)\n",
    "\n",
    "# Verify the final predictions\n",
    "print(final_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8487fd61-06b2-4480-9f1c-fe96c71e8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id        F1        F2        F3        F4        F5  \\\n",
      "0  HOUSEHOLD_1_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "0  HOUSEHOLD_2_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "0      FOODS_1_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "0      FOODS_2_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "0      FOODS_3_validation  1.362097  1.362097  1.362097  1.362097  1.362097   \n",
      "\n",
      "         F6        F7        F8        F9  ...       F19       F20       F21  \\\n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "0  1.362671  1.680848  1.956718  1.928068  ...  1.334594  1.611038  1.582388   \n",
      "\n",
      "        F22       F23       F24       F25       F26       F27       F28  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "0  1.363818  1.363818  1.363818  1.363818  1.363818  1.364392  1.364392  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(final_predictions.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec696cf-4673-41a9-9192-a8cb89df06f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
